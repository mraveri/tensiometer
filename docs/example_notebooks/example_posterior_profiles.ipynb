{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient posterior profiles\n",
    "\n",
    "Shivam Pandey (), Cyrille Doux (<doux@lpsc.in2p3.fr>), Marco Raveri (<marco.raveri@unige.it>)\n",
    "\n",
    "In this notebook we show how to obtain posterior profiles from synthetic probability models, as in [Pandey, Doux and Raveri (2024), arXiv:XXXX.XXXX](https://arxiv.org/abs/XXXX.XXXXX).\n",
    "\n",
    "If you want more details on how to build normalizing flow based synthetic models for posterior distributions check out the corresponding example notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook setup:\n",
    "\n",
    "We start by importing everything and setting up a controlled example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show plots inline, and load main getdist plot module and samples class\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import libraries:\n",
    "import sys, os\n",
    "sys.path.insert(0,os.path.realpath(os.path.join(os.getcwd(),'../..')))\n",
    "from getdist import plots, MCSamples\n",
    "from getdist.gaussian_mixtures import GaussianND\n",
    "import getdist\n",
    "getdist.chains.print_load_details = False\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# tensorflow imports:\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "# import the tensiometer tools that we need:\n",
    "import tensiometer\n",
    "from tensiometer import utilities\n",
    "from tensiometer import synthetic_probability\n",
    "\n",
    "# getdist settings to ensure consistency of plots:\n",
    "getdist_settings = {'ignore_rows': 0.0, \n",
    "                    'smooth_scale_2D': 0.3,\n",
    "                    'smooth_scale_1D': 0.3,\n",
    "                    }    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build here a random Gaussian mixture model that we are going to use for tests.\n",
    "\n",
    "The esample is seeded so that it is reproducible. If you want a different example change the value of the seed.\n",
    "\n",
    "You can also change dimensionality and number of modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameters of the problem:\n",
    "dim = 6\n",
    "num_gaussians = 3\n",
    "num_samples = 10000\n",
    "\n",
    "# we seed the random number generator to get reproducible results:\n",
    "seed = 100\n",
    "np.random.seed(seed)\n",
    "# we define the range for the means and covariances:\n",
    "mean_range = (-0.5, 0.5)\n",
    "cov_scale = 0.4**2\n",
    "# generate means and covs:\n",
    "means = np.random.uniform(mean_range[0], mean_range[1], num_gaussians*dim).reshape(num_gaussians, dim)\n",
    "weights = np.random.rand(num_gaussians)\n",
    "weights = weights / np.sum(weights)\n",
    "covs = [cov_scale*utilities.vector_to_PDM(np.random.rand(int(dim*(dim+1)/2))) for _ in range(num_gaussians)]\n",
    "\n",
    "# initialize distribution:\n",
    "distribution = tfp.distributions.Mixture(\n",
    "    cat=tfp.distributions.Categorical(probs=weights),\n",
    "    components=[\n",
    "        tfp.distributions.MultivariateNormalTriL(loc=_m, scale_tril=tf.linalg.cholesky(_c))\n",
    "        for _m, _c in zip(means, covs)\n",
    "    ], name='Mixture')\n",
    "\n",
    "# sample the distribution:\n",
    "samples = distribution.sample(num_samples).numpy()\n",
    "# calculate log posteriors:\n",
    "logP = distribution.log_prob(samples).numpy()\n",
    "\n",
    "# create MCSamples from the samples:\n",
    "chain = MCSamples(samples=samples, \n",
    "                  settings=getdist_settings,\n",
    "                  loglikes=-logP,\n",
    "                  name_tag='Mixture',\n",
    "                  )\n",
    "\n",
    "# we want to find the maximum posterior point:\n",
    "_temp_maxima, _temp_max_value = [], []\n",
    "for _m in means:\n",
    "    # maximize the likelihood starting from all the means:\n",
    "    _max = scipy.optimize.minimize(lambda x: -distribution.log_prob(x).numpy(), _m, method='Nelder-Mead')\n",
    "    # this usually converges to the nearest mode:\n",
    "    _temp_maxima.append(_max.x)\n",
    "    _temp_max_value.append(-_max.fun)\n",
    "maximum = _temp_maxima[np.argmax(_temp_max_value)]\n",
    "maximum_value = _temp_max_value[np.argmax(_temp_max_value)]\n",
    "\n",
    "# we make a sanity check plot:\n",
    "g = plots.get_subplot_plotter()\n",
    "g.triangle_plot(chain, filled=True, markers=maximum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see this example beautifully showcases projection effects, especially in $p_5$. As you might have seen in the synthetic probability notebook the low peak in the 1D marginal of $p_5$ is the actual full-D peak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flow training:\n",
    "\n",
    "We now train a synthetic probability model. Note that we need a flow with good local accuracy since we are going to maximize its value.\n",
    "\n",
    "If you are interested in how to build and control these types of flows, check out the synthetic probability tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "          'feedback': 1,\n",
    "          'verbose': -1,\n",
    "          'plot_every': 1000,\n",
    "          'pop_size': 1,\n",
    "          'num_flows': 5,\n",
    "          'epochs': 300,\n",
    "        }\n",
    "\n",
    "flow = tensiometer.synthetic_probability.average_flow_from_chain(chain,  # parameter difference chain\n",
    "                                                                 **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to check metrics to make sure everything looks good:\n",
    "flow.print_training_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we need to check the estimate of the local accuracy:\n",
    "ev, eer = flow.evidence()\n",
    "print(f'log(Z) = {ev} +- {eer}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is usually good to check that the evidence error (if available) is under control, especially when we want to obtain posterior profiles. \n",
    "\n",
    "The evidence error is the variance of log likelihood values on samples from the flow and gives us a handle on the local accuracy of the flow.\n",
    "\n",
    "Cathastrofic initialization of weights happens and if this value is too high then it might be worth re-running flow training.\n",
    "\n",
    "If you are training on marginals (without likelihood values available) then it might be a good idea to add population selection as a layer of protection against bad initialization.\n",
    "\n",
    "If you find that the results are not stable (especially in the bulk of the posterior) check out the notebook on synthetic probability modelling and the section discussing high accuracy flows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior profile:\n",
    "\n",
    "We now want to calculate posterior profiles for our distribution. These are obtained maximizing over all parameters but the ones that are been considered.\n",
    "\n",
    "Having efficient flow models from which we can sample and calculate probability values means that we can afford lots of maximizations, so that we can calculate up to 2D profiles. \n",
    "\n",
    "Note that a flow can be trained on marginal distributions, allowing us to combine profiling and marginalization.\n",
    "\n",
    "Posterior profiles can be easily obtained using the appropriate tensiometer class operating on the flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensiometer.synthetic_probability import flow_profiler\n",
    "\n",
    "# define the options for the profiler:\n",
    "profiler_options = {\n",
    "        'num_gd_interactions_1D': 100,  # number of gradient descent interactions for the 1D profile\n",
    "        'num_gd_interactions_2D': 100,  # number of gradient descent interactions for the 2D profile\n",
    "        'scipy_options': {  # options for the scipy polishing minimizer\n",
    "                    'ftol': 1.e-06,\n",
    "                    'gtol': 0.0,\n",
    "                    'maxls': 40,\n",
    "                },\n",
    "        'scipy_use_jac': True,  # use the jacobian in the minimizer\n",
    "        'num_points_1D': 64, # number of points for the 1D profile\n",
    "        'num_points_2D': 32, # number of points per dimension for the 2D profile\n",
    "        'smooth_scale_1D': 0.2, # smoothing scale for the 1D profile\n",
    "        'smooth_scale_2D': 0.2, # smoothing scale for the 2D profile\n",
    "        }\n",
    "\n",
    "# initialize the profiler:\n",
    "flow_profile = flow_profiler.posterior_profile_plotter(flow, \n",
    "                                                       initialize_cache=False,  # usually we want to avoid initializing the cache for all the parameters\n",
    "                                                       feedback=2,  # we want high feedback to see the progress\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have initialized the profiler. If cache initialization is enabled the code will calculate 1D and 2D profiles for all parameter combinations. This is usually a lot of work so we keep it separate in this tutorial and proceed with the profile calculation for just a few parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we initialize the cache for the parameters we want to profile:\n",
    "profile_params = ['param3', 'param5']\n",
    "flow_profile.update_cache(params=profile_params, \n",
    "                          **profiler_options)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can notice the profile calculation is complicated and intensive. For this reason caching is implemented and thoroughly used.\n",
    "\n",
    "After calculating profiles the result can be saved to file for effective caching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this command will save the profile to a pickle file:\n",
    "#flow_profile.savePickle('flow_profile.pkl')\n",
    "# note that the flow cannot be pickled easily and has its own save and load functions. This means you have to save it separately.\n",
    "#flow_profile = flow_profiler.posterior_profile_plotter.loadPickle('flow_profile.pkl', flow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The profiler class hijacks getdist MCSamples so that it can be directly used for getdist plotting as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = plots.get_subplot_plotter()\n",
    "g.triangle_plot([flow_profile, flow.MCSamples(10000)], \n",
    "                params=profile_params,\n",
    "                markers=[flow_profile.flow_MAP[flow_profile.index[_p]] for _p in profile_params],\n",
    "                filled=False, \n",
    "                shaded=True, \n",
    "                diag1d_kwargs={'normalized':True},\n",
    "                legend_labels=['Profile','Marginal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the projection effect in $p_5$ is rightfully recovered and the mode that is smaller in the marginal is much higher in the profile.\n",
    "\n",
    "Note that there might be some little discrepancy between the peak of the profiles and the full-D peak, while there should be no difference.\n",
    "This is due to the finite resolution of the 1D and 2D profiles, which are typically only computed on a small-ish grid.\n",
    "\n",
    "The profiler class is fully interfaced with getdist plotting facilities. \n",
    "Everything that is not previously cached is recomputed on the flight, as we can see in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_params = ['param1'] + profile_params \n",
    "g = plots.get_subplot_plotter()\n",
    "g.plots_1d([flow_profile, flow.MCSamples(10000)], \n",
    "           params=plot_params, \n",
    "           legend_labels=['Profile','Marginal'], \n",
    "           nx=3, normalized=True,\n",
    "           markers=[flow_profile.flow_MAP[flow_profile.index[_p]] for _p in plot_params])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile accuracy tests:\n",
    "\n",
    "The profiler calculation is hard. As you might have seen it requires hundreds or thousands of minimization instances. \n",
    "\n",
    "In this section we investigate the reliability of the profile calculation. \n",
    "We can do so since - in this example - we have the exact distribution available.\n",
    "\n",
    "We implemented methods to wrap a tensorflow or scipy distribution into a flow so that it can be used for thorough tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case we can compare with the profile obtained from the exact distribution:\n",
    "\n",
    "from tensiometer.synthetic_probability import analytic_flow\n",
    "\n",
    "exact_flow = analytic_flow.analytic_flow(analytic_flow.tf_prob_wrapper(distribution),\n",
    "                           param_names=flow.param_names, \n",
    "                           param_labels=flow.param_labels, \n",
    "                           lims=flow.parameter_ranges)\n",
    "exact_profile = flow_profiler.posterior_profile_plotter(exact_flow, \n",
    "                                          initialize_cache=False,\n",
    "                                          feedback=2 )\n",
    "exact_profile.update_cache(params=profile_params, \n",
    "                            **profiler_options)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot the profiles.\n",
    "\n",
    "We log plot the 1D distributions to appreciate flow accuracy in the tails. Note that the flow is optimized on log probabilities so it is expected to do a fairly good job across orders of magnitudes in probability. On the other hand small errors in log space are big errors in probability, hence the requirement of high overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = plots.get_subplot_plotter()\n",
    "g.triangle_plot([flow_profile, exact_profile], \n",
    "                params=profile_params,\n",
    "                filled=False, \n",
    "                shaded=True, \n",
    "                diag1d_kwargs={'normalized':True},\n",
    "                markers=[exact_profile.flow_MAP[flow_profile.index[_p]] for _p in profile_params],\n",
    "                legend_labels=['Flow Profile','Exact Profile'])\n",
    "# log axis on the diagonal:\n",
    "for _i in range(len(profile_params)):\n",
    "    _ax = g.subplots[_i, _i]\n",
    "    _ax.set_yscale('log')\n",
    "    _ax.set_ylim(1.e-5, 1.e1)\n",
    "    _ax.set_ylabel('$\\\\log_{10}(P)$')\n",
    "    _ax.tick_params(axis='y', which='both', labelright=True)\n",
    "    _ax.yaxis.set_label_position('right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we trained average flows we can also estimate the error on the profile as the variance of the logPs across the flows. This is what we are going to do here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = plots.get_subplot_plotter()\n",
    "g.triangle_plot([flow_profile, exact_profile], params=profile_params,\n",
    "                filled=False, shaded=True, diag1d_kwargs={'normalized':True},\n",
    "                markers=[flow_profile.flow_MAP[flow_profile.index[_p]] for _p in profile_params],\n",
    "                legend_labels=['Flow Profile','Exact Profile'])\n",
    "\n",
    "# add error bar on the diagonal:\n",
    "for _i in range(len(profile_params)):\n",
    "    # call the method that computes the variance of the profile for an average flow:\n",
    "    _x, _prob, _temp_std = flow_profile.get_1d_profile_variance(profile_params[_i])\n",
    "    # do the plotting:\n",
    "    _ax = g.subplots[_i, _i]\n",
    "    _ax.plot(_x, _prob, color='k', linestyle='-', label='True')\n",
    "    _ax.fill_between(_x, _prob - _temp_std, _prob + _temp_std, color='k', alpha=0.2)\n",
    "    _ax.set_ylabel('$P$')\n",
    "    _ax.tick_params(axis='y', which='both', labelright=True)\n",
    "    _ax.yaxis.set_label_position('right')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real world application: cosmological parameter profiles\n",
    "\n",
    "In this section we show a real example of a profile applied to cosmological parameter posteriors.\n",
    "\n",
    "In this case it is particularly interesting to combine profiling and marginalization. The full parameter space of the model is large - of order 30D - but most of these parameters describe systematic effects. These can be marginalized over, after all we might not really be interested in what happens with them and then we can profile cosmological parameters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we start by loading up the posterior:\n",
    "\n",
    "# load the samples (remove no burn in since the example chains have already been cleaned):\n",
    "chains_dir = './../../test_chains/'\n",
    "# the DES Y1 3x2 chain:\n",
    "data_chain = getdist.mcsamples.loadMCSamples(file_root=chains_dir+'DES', no_cache=True, settings=getdist_settings)\n",
    "\n",
    "# let's add omegab as a derived parameter:\n",
    "for _ch in [data_chain]:\n",
    "    _p = _ch.getParams()\n",
    "    _h = _p.H0 / 100.\n",
    "    _ch.addDerived(_p.omegabh2 / _h**2, name='omegab', label='\\\\Omega_b')\n",
    "    _ch.updateBaseStatistics()\n",
    "\n",
    "# we define the parameters of the problem:\n",
    "param_names = ['H0', 'omegam', 'sigma8', 'ns', 'omegab']\n",
    "\n",
    "# we then train the flows on the base parameters that we want to use:\n",
    "kwargs = {\n",
    "          'feedback': 1,\n",
    "          'verbose': -1,\n",
    "          'plot_every': 1000,\n",
    "          'pop_size': 1,\n",
    "          'num_flows': 5,\n",
    "          'epochs': 500,\n",
    "        }\n",
    "\n",
    "# actual flow training:\n",
    "data_flow = tensiometer.synthetic_probability.average_flow_from_chain(data_chain, param_names=param_names, **kwargs)\n",
    "\n",
    "# plot to make sure training went well:\n",
    "data_flow.training_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check triangle plot:\n",
    "g = plots.get_subplot_plotter()\n",
    "g.triangle_plot([data_chain, data_flow.MCSamples(20000, settings=getdist_settings),\n",
    "                 ], \n",
    "                params=param_names,\n",
    "                filled=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the options for the profiler:\n",
    "profiler_options = {\n",
    "        'num_gd_interactions_1D': 100,  # number of gradient descent interactions for the 1D profile\n",
    "        'num_gd_interactions_2D': 100,  # number of gradient descent interactions for the 2D profile\n",
    "        'scipy_options': {  # options for the polishing minimizer\n",
    "                    'ftol': 1.e-06,\n",
    "                    'gtol': 0.0,\n",
    "                    'maxls': 100,\n",
    "                },\n",
    "        'scipy_use_jac': True,  # use the jacobian in the minimizer\n",
    "        'num_points_1D': 64, # number of points for the 1D profile\n",
    "        'num_points_2D': 32, # number of points per dimension for the 2D profile\n",
    "        'smooth_scale_1D': 0.2, # smoothing scale for the 1D profile\n",
    "        'smooth_scale_2D': 0.2, # smoothing scale for the 2D profile\n",
    "        }\n",
    "\n",
    "# initialize the profiler:\n",
    "data_flow_profile = flow_profiler.posterior_profile_plotter(data_flow, \n",
    "                                                            initialize_cache=False,  # usually we want to avoid initializing the cache for all the parameters\n",
    "                                                            feedback=2,  # we want high feedback to see the progress\n",
    "                                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we initialize the cache for the parameters we want to profile:\n",
    "profile_params = ['omegam', 'sigma8', 'ns']\n",
    "data_flow_profile.update_cache(params=profile_params, \n",
    "                               **profiler_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the profiles.\n",
    "\n",
    "Note that - in this case - since we have no evidence estimate available it is crucial to train a bunch of flows to get an estimate of the variance of the profile.\n",
    "If the variance is large it is usually a good idea to retrain..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = plots.get_subplot_plotter()\n",
    "g.triangle_plot([data_flow_profile, data_flow.MCSamples(10000)], params=profile_params,\n",
    "                filled=False, shaded=True, diag1d_kwargs={'normalized':True},\n",
    "                markers=[data_flow_profile.flow_MAP[data_flow_profile.index[_p]] for _p in profile_params],\n",
    "                legend_labels=['Profile','Marginal'])\n",
    "\n",
    "# add error bar on the diagonal:\n",
    "for _i in range(len(profile_params)):\n",
    "    _x, _prob, _temp_std = data_flow_profile.get_1d_profile_variance(profile_params[_i])\n",
    "    # do the plotting:\n",
    "    _ax = g.subplots[_i, _i]\n",
    "    _ax.plot(_x, _prob, color='k', linestyle='-', label='True')\n",
    "    _ax.fill_between(_x, _prob - _temp_std, _prob + _temp_std, color='k', alpha=0.2)\n",
    "    _ax.set_ylabel('$P$')\n",
    "    _ax.tick_params(axis='y', which='both', labelright=True)\n",
    "    _ax.yaxis.set_label_position('right')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2D profiles are pretty expensive, 1D profiles, on the other hand, are fairly fast, to the point that we can compute them all for this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = plots.get_subplot_plotter()\n",
    "data_flow_profile.normalize()\n",
    "g.plots_1d([data_flow_profile, data_flow.MCSamples(10000)], \n",
    "           params=param_names, \n",
    "           legend_labels=['Profile','Marginal'],\n",
    "           markers=[data_flow_profile.flow_MAP[data_flow_profile.index[_p]] for _p in param_names], \n",
    "           nx=5, share_y=True, normalize=False)\n",
    "\n",
    "# add error bars:\n",
    "for _i in range(len(param_names)):\n",
    "    _x, _prob, _temp_std = data_flow_profile.get_1d_profile_variance(param_names[_i], normalize_by='max')\n",
    "    # do the plotting:\n",
    "    _ax = g.subplots.flatten()[_i]\n",
    "    _ax.plot(_x, _prob, color='k', linestyle='-', label='True')\n",
    "    _ax.fill_between(_x, _prob - _temp_std, _prob + _temp_std, color='k', alpha=0.2)\n",
    "    _ax.set_ylabel('$P$')\n",
    "    _ax.tick_params(axis='y', which='both', labelright=True)\n",
    "    _ax.yaxis.set_label_position('right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compute all 1D profiles we can get best-fit and error bars from the profile likelihood (posterior really) ratios.\n",
    "These are defined from posterior thresholds from the maximum.\n",
    "\n",
    "To do so we have hijacked getdist method getLikeStats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likestats = data_flow_profile.getLikeStats()\n",
    "print(likestats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These can be compared to the original margestats. Note that these are obtained starting from the original chain that was fed to the flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margestats = data_flow.MCSamples(10000).getMargeStats()\n",
    "print(margestats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use them to visualize constraints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = plots.get_subplot_plotter()\n",
    "data_flow_profile.normalize()\n",
    "g.plots_1d([data_flow_profile, data_flow.MCSamples(10000)], \n",
    "           params=param_names, legend_labels=['Profile','Marginal'],\n",
    "           markers=[data_flow_profile.flow_MAP[data_flow_profile.index[_p]] for _p in param_names], \n",
    "           nx=5, share_y=True, normalize=False)\n",
    "\n",
    "# add error bars:\n",
    "for _i in range(len(param_names)):\n",
    "    _ax = g.subplots.flatten()[_i]\n",
    "    _marge = margestats.parWithName(param_names[_i])\n",
    "    _like = likestats.parWithName(param_names[_i])\n",
    "    _ax.axvspan(_marge.limits[0].lower, _marge.limits[0].upper, color='r', alpha=0.2)\n",
    "    _ax.axvspan(_like.ND_limit_bot[0], _like.ND_limit_top[0], color='k', alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that - in this example - confidence intervals obtained from the profile are more conservative than those obained with marginalized statistics.\n",
    "\n",
    "Having hijacked getdist MCSamples, if we query for MargeStats the profiler we will get confidence intervals calculated with a given mass threshold (i.e. that the isocontour should integrate to a fraction of total).\n",
    "\n",
    "Let's compare the two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likestats = data_flow_profile.getLikeStats()\n",
    "margestats = data_flow_profile.getMargeStats()\n",
    "\n",
    "g = plots.get_subplot_plotter()\n",
    "data_flow_profile.normalize()\n",
    "g.plots_1d([data_flow_profile], \n",
    "           params=param_names, legend_labels=['Profile'],\n",
    "           markers=[data_flow_profile.flow_MAP[data_flow_profile.index[_p]] for _p in param_names], \n",
    "           nx=5, share_y=True, normalize=False)\n",
    "\n",
    "# add error bars:\n",
    "for _i in range(len(param_names)):\n",
    "    _ax = g.subplots.flatten()[_i]\n",
    "    _marge = margestats.parWithName(param_names[_i])\n",
    "    _like = likestats.parWithName(param_names[_i])\n",
    "    _ax.axvspan(_marge.limits[0].lower, _marge.limits[0].upper, color='r', alpha=0.2)\n",
    "    _ax.axvspan(_like.ND_limit_bot[0], _like.ND_limit_top[0], color='k', alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the likelihood threshold is still more conservative. Note that this is a sign of a non-Gaussian distribution and specific to this case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
